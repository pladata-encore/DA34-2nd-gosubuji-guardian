{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae833fe-518d-4c5d-9e60-45dedac70c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random  \n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image, clear_output\n",
    "from torch.cuda import memory_allocated, empty_cache\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2163364-85c1-4a65-8fa9-7b3859c717fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kimsungwook/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ğŸš€ 2024-4-17 Python-3.11.7 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# torch_ver Yolov5\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s',\n",
    "                            device='cuda:0' if torch.cuda.is_available() else 'cpu')  # ì˜ˆì¸¡ ëª¨ë¸\n",
    "yolo_model.classes = [0]  # ì˜ˆì¸¡ í´ë˜ìŠ¤ (0 : ì‚¬ëŒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28a20db-09d0-4a17-a818-ae3ba02bd6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "class skeleton_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(skeleton_LSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=len(attention_dot) * 2, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm5 = nn.LSTM(input_size=256, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.fc = nn.Linear(32,2)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x, _ = self.lstm5(x)\n",
    "        x, _ = self.lstm6(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm7(x)\n",
    "        x = self.fc(x[:,-1,:]) # x[ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´, ì€ë‹‰ ìƒíƒœ í¬ê¸°], [:, -1, :] -> ë§ˆì§€ë§‰ ì‹œê°„ ë‹¨ê³„ë§Œ ì„ íƒ\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ef6941-86c9-4e85-9c49-7fb4f8f2809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose : Only Body\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 6\n",
    "EPOCH = 300\n",
    "NUM_LAYERS = 3      # LSTM model: num_layers\n",
    "start_dot = 11      # mp.solutions.pose ì‹œì‘ í¬ì¸íŠ¸ (0: ì–¼êµ´ë¶€í„° ë°œëª©ê¹Œì§€, 11: ì–´ê¹¨ë¶€í„° ë°œëª©ê¹Œì§€)\n",
    "n_CONFIDENCE = 0.3    # MediaPipe Min Detectin confidence check\n",
    "y_CONFIDENCE = 0.3    # Yolv5 Min Detectin confidence check\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "attention_dot = [n for n in range(start_dot, 29)]\n",
    "\n",
    "# ë¼ì¸ ê·¸ë¦¬ê¸°\n",
    "if start_dot == 11:\n",
    "    \"\"\"ëª¸ ë¶€ë¶„ë§Œ\"\"\"\n",
    "    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \\\n",
    "                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \\\n",
    "                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \\\n",
    "                [23, 24], [12, 24]]\n",
    "    print('Pose : Only Body')\n",
    "\n",
    "else:\n",
    "    \"\"\"ì–¼êµ´ í¬í•¨\"\"\"\n",
    "    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \\\n",
    "                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \\\n",
    "                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \\\n",
    "                [23, 24], [12, 24], [9, 10], [0, 5], [0, 2], [5, 8], [2, 7]]\n",
    "    print('Pose : Face + Body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31254c0e-5ecf-4de6-bc1a-260ccec34d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = skeleton_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4f8a16-db5b-45cb-bb11-e7d8c7fcaa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('LSTM1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "895e393a-3514-418c-b20f-229b55235b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolov4 ë°”ìš´ë”© box ì•ˆì—ì„œ media pipe ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "\n",
    "def get_skeleton(video_path, attention_dot, draw_line):\n",
    "    frame_length = 30 # LSTM ëª¨ë¸ì— ë„£ì„ frame ìˆ˜\n",
    "\n",
    "    xy_list_list, xy_list_list_flip = [], []\n",
    "    cv2.destroyAllWindows()\n",
    "    pose = mp_pose.Pose(static_image_mode = True, model_complexity = 1, \\\n",
    "                        enable_segmentation = False, min_detection_confidence = n_CONFIDENCE)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if cap.isOpened():\n",
    "\n",
    "        while True:\n",
    "            ret, img = cap.read()\n",
    "\n",
    "            if ret == True:\n",
    "\n",
    "                \"\"\" Yolo ë°”ìš´ë”© ë°•ìŠ¤ ë° ì¢Œí‘œ ì¶”ì¶œ\"\"\"\n",
    "                img = cv2.resize(img, (640, 640))\n",
    "                res = yolo_model(img)\n",
    "                res_refine = res.pandas().xyxy[0].values\n",
    "                nms_human = len(res_refine)\n",
    "                if nms_human > 0:\n",
    "                    for bbox in res_refine:\n",
    "                        \"\"\"ë°”ìš´ë”© ë°•ìŠ¤ ìƒí•˜ì¢Œìš° í¬ê¸° ì¡°ì ˆ\"\"\"\n",
    "                        xx1, yy1, xx2, yy2 = int(bbox[0])-10, int(bbox[1]), int(bbox[2])+10, int(bbox[3])\n",
    "                        if xx1 < 0:\n",
    "                            xx1 = 0\n",
    "                        elif xx2 > 639:\n",
    "                            xx2 = 639\n",
    "                        if yy1 < 0:\n",
    "                            yy1 = 0\n",
    "                        elif yy2 > 639:\n",
    "                            yy2 = 639\n",
    "\n",
    "                        start_point = (xx1, yy1)\n",
    "                        end_point = (xx2, yy2)\n",
    "\n",
    "                        \"\"\" Yolov5 ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ì•ˆì—ì„œ mediapipe Pose ì¶”ì¶œ\"\"\"\n",
    "                        if bbox[4] > y_CONFIDENCE: # bbox[4] : confidence ë°ì´í„°\n",
    "                            # img = cv2.rectangle(img, start_point, end_point, (0, 0, 255), 2) # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° : ë°ì´í„° ì¶”ì¶œ í™•ì¸ìš©\n",
    "                            c_img = img[yy1:yy2, xx1:xx2] # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ\n",
    "                            results = pose.process(cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)) # Yolov5 ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ì•ˆì—ì„œ 'mp_pose' ì¢Œí‘œ\n",
    "\n",
    "                            if not results.pose_landmarks: continue\n",
    "                            idx = 0\n",
    "                            draw_line_dic = {}\n",
    "                            xy_list, xy_list_flip = [], []\n",
    "                            # 33 ë°˜ë³µë¬¸ ì§„í–‰ : 33ê°œ ì¤‘ 18ê°œì˜ dot\n",
    "                            for x_and_y in results.pose_landmarks.landmark:\n",
    "                                if idx in attention_dot:\n",
    "                                    xy_list.append(x_and_y.x)\n",
    "                                    xy_list.append(x_and_y.y)\n",
    "                                    xy_list_flip.append(1 - x_and_y.x)\n",
    "                                    xy_list_flip.append(x_and_y.y)\n",
    "\n",
    "                                    x, y = int(x_and_y.x*(xx2-xx1)), int(x_and_y.y*(yy2-yy1))\n",
    "                                    draw_line_dic[idx] = [x, y]\n",
    "                                idx += 1\n",
    "\n",
    "                            if len(xy_list) != len(attention_dot) * 2:\n",
    "                                print('Error : attention_dot ë°ì´í„° ì˜¤ë¥˜')\n",
    "\n",
    "                            xy_list_list.append(xy_list)\n",
    "                            xy_list_list_flip.append(xy_list_flip)\n",
    "\n",
    "                            \"\"\"mediapipe line ê·¸ë¦¬ê¸° ë¶€ë¶„ : ë°ì´í„° ì¶”ì¶œ(dot) í™•ì¸ìš©\"\"\"\n",
    "                            # for line in draw_line:\n",
    "                            #     x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
    "                            #     x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
    "                            #     c_img = cv2.line(c_img, (x1, y1), (x2, y2), (0, 255, 0), 4)\n",
    "                            # # cv2.imshow('Landmark Image', img)\n",
    "                            # cv2_imshow(img)\n",
    "                            # cv2.waitKey(1)\n",
    "\n",
    "            elif ret == False: break\n",
    "\n",
    "\n",
    "        # ë¶€ì¡±í•œ í”„ë ˆì„ ìˆ˜ ë§ì¶”ê¸°\n",
    "        if len(xy_list_list_flip) < 15:\n",
    "            return False, False\n",
    "        elif len(xy_list_list_flip) < frame_length:\n",
    "            f_ln = frame_length - len(xy_list_list_flip)\n",
    "            for _ in range(f_ln):\n",
    "                xy_list_list.append(xy_list_list[-1])\n",
    "                xy_list_list_flip.append(xy_list_list_flip[-1])\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    return xy_list_list, xy_list_list_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d950f5-caaa-495a-ac7d-df31d500b043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ì¥ëœ frameì˜ ê°œìˆ˜: 150\n"
     ]
    }
   ],
   "source": [
    "# ì˜ìƒ resize ë° ì¶”ì¶œ\n",
    "#test_video_name = 'C_3_12_43_BU_SMC_10-14_12-17-14_CC_RGB_DF2_F2'\n",
    "test_video_path = f'01_pocket_30.mp4'\n",
    "cv2.destroyAllWindows()\n",
    "cap = cv2.VideoCapture(test_video_path)\n",
    "img_list = []\n",
    "\n",
    "if cap.isOpened():\n",
    "\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if ret:\n",
    "            img = cv2.resize(img, (640, 640))\n",
    "            img_list.append(img)\n",
    "            # cv2_imshow(img)\n",
    "            # cv2.waitKey(1)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print('ì €ì¥ëœ frameì˜ ê°œìˆ˜: {}'.format(len(img_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61fde9f9-6857-4f15-a3cd-fb5e5bbaa30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, seq_list):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        for dic in seq_list :\n",
    "            self.y.append(dic['key'])\n",
    "            self.X.append(dic['value'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.X[index]\n",
    "        label = self.y[index]\n",
    "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd51a595-6bda-4395-8b16-b19889748773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43474f93-f0ac-44f4-ac68-4d59750051a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713832264.506778 1408120 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‹œí€€ìŠ¤ ë°ì´í„° ë¶„ì„ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:17<00:00,  8.67it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Yolov5 + Mediapipe Version\"\"\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "length = 30 # frame ìƒíƒœë¥¼ í‘œì‹œí•  ê¸¸ì´\n",
    "out_img_list = []\n",
    "dataset = []\n",
    "status = 'None'\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1, enable_segmentation=False, min_detection_confidence=n_CONFIDENCE)\n",
    "print('ì‹œí€€ìŠ¤ ë°ì´í„° ë¶„ì„ ì¤‘...')\n",
    "\n",
    "xy_list_list = []\n",
    "for img in tqdm(img_list):\n",
    "    res = yolo_model(img)\n",
    "    res_refine = res.pandas().xyxy[0].values\n",
    "\n",
    "    nms_human = len(res_refine)\n",
    "    if nms_human > 0:\n",
    "        for bbox in res_refine:\n",
    "            xx1, yy1, xx2, yy2 = int(bbox[0])-10, int(bbox[1]), int(bbox[2])+10, int(bbox[3])\n",
    "            if xx1 < 0:\n",
    "                xx1 = 0\n",
    "            elif xx2 > 639:\n",
    "                xx2 = 639\n",
    "            if yy1 < 0:\n",
    "                yy1 = 0\n",
    "            elif yy2 > 639:\n",
    "                yy2 = 639\n",
    "\n",
    "            start_point = (xx1, yy1)\n",
    "            end_point = (xx2, yy2)\n",
    "            if bbox[4] > y_CONFIDENCE:\n",
    "                img = cv2.rectangle(img, start_point, end_point, (0, 0, 255), 2)\n",
    "\n",
    "                c_img = img[yy1:yy2, xx1:xx2]\n",
    "                results = pose.process(cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)) # Yolo ë°”ìš´ë”© box ì•ˆì—ì„œ landmark dot ì¶”ì¶œ\n",
    "                if not results.pose_landmarks: continue\n",
    "                xy_list = []\n",
    "                idx = 0\n",
    "                draw_line_dic = {}\n",
    "                for x_and_y in results.pose_landmarks.landmark:\n",
    "                    if idx in attention_dot:\n",
    "                        xy_list.append(x_and_y.x)\n",
    "                        xy_list.append(x_and_y.y)\n",
    "                        x, y = int(x_and_y.x*(xx2-xx1)), int(x_and_y.y*(yy2-yy1))\n",
    "                        draw_line_dic[idx] = [x, y]\n",
    "                    idx += 1\n",
    "\n",
    "                xy_list_list.append(xy_list)\n",
    "                for line in draw_line:\n",
    "                    x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
    "                    x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
    "                    c_img = cv2.line(c_img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "                if len(xy_list_list) == length:\n",
    "                    dataset = []\n",
    "                    dataset.append({'key' : 0, 'value' : xy_list_list})\n",
    "                    dataset = MyDataset(dataset)\n",
    "                    dataset = DataLoader(dataset)\n",
    "                    xy_list_list = []\n",
    "\n",
    "                    for data, label in dataset:\n",
    "                        data = data.to(device)\n",
    "                        with torch.no_grad():\n",
    "                            result = model(data)\n",
    "                            _, out = torch.max(result, 1)\n",
    "                            if out.item() == 0: status = 'Normal'\n",
    "                            else: status = 'Theft'\n",
    "\n",
    "    cv2.putText(img, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 0, 255), 2)\n",
    "    out_img_list.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ed2f6f7-dbec-4205-944b-5cdc9e44f213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì›ë³¸ ì˜ìƒ ë‚´ë³´ë‚´ê¸°\n",
    "filename = './output.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "fps = 3\n",
    "frameSize = (640, 640)\n",
    "isColor = True\n",
    "out = cv2.VideoWriter(filename, fourcc, fps, frameSize, isColor)\n",
    "for out_img in out_img_list:\n",
    "    out.write(out_img)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb8a5c-4387-4f66-ac14-c4fe9e954da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo5test",
   "language": "python",
   "name": "yolo5test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
