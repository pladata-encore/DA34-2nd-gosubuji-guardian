{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random  \n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image, clear_output\n",
    "from torch.cuda import memory_allocated, empty_cache\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yolov5 torch_버전 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kimsungwook/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2024-4-17 Python-3.11.7 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# torch_ver Yolov5\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s',\n",
    "                            device='cuda:0' if torch.cuda.is_available() else 'cpu')  # 예측 모델\n",
    "yolo_model.classes = [0]  # 예측 클래스 (0 : 사람)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 전처리 및 세부 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose : Only Body\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 40\n",
    "EPOCH = 700\n",
    "NUM_LAYERS = 1      # LSTM model: num_layers\n",
    "start_dot = 11      # mp.solutions.pose 시작 포인트 (0: 얼굴부터 발목까지, 11: 어깨부터 발목까지)\n",
    "n_CONFIDENCE = 0.3    # MediaPipe Min Detectin confidence check\n",
    "y_CONFIDENCE = 0.3    # Yolv5 Min Detectin confidence check\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "attention_dot = [n for n in range(start_dot, 29)]\n",
    "\n",
    "# 라인 그리기\n",
    "if start_dot == 11:\n",
    "    \"\"\"몸 부분만\"\"\"\n",
    "    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \\\n",
    "                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \\\n",
    "                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \\\n",
    "                [23, 24], [12, 24]]\n",
    "    print('Pose : Only Body')\n",
    "\n",
    "else:\n",
    "    \"\"\"얼굴 포함\"\"\"\n",
    "    draw_line = [[11, 13], [13, 15], [15, 21], [15, 19], [15, 17], [17, 19], \\\n",
    "                [12, 14], [14, 16], [16, 22], [16, 20], [16, 18], [18, 20], \\\n",
    "                [23, 25], [25, 27], [24, 26], [26, 28], [11, 12], [11, 23], \\\n",
    "                [23, 24], [12, 24], [9, 10], [0, 5], [0, 2], [5, 8], [2, 7]]\n",
    "    print('Pose : Face + Body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolov4 바운딩 box 안에서 media pipe 데이터 전처리 함수\n",
    "\n",
    "def get_skeleton(video_path, attention_dot, draw_line):\n",
    "    frame_length = 30 # LSTM 모델에 넣을 frame 수\n",
    "\n",
    "    xy_list_list, xy_list_list_flip = [], []\n",
    "    cv2.destroyAllWindows()\n",
    "    pose = mp_pose.Pose(static_image_mode = True, model_complexity = 1, \\\n",
    "                        enable_segmentation = False, min_detection_confidence = n_CONFIDENCE)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if cap.isOpened():\n",
    "\n",
    "        while True:\n",
    "            ret, img = cap.read()\n",
    "\n",
    "            if ret == True:\n",
    "\n",
    "                \"\"\" Yolo 바운딩 박스 및 좌표 추출\"\"\"\n",
    "                img = cv2.resize(img, (640, 640))\n",
    "                res = yolo_model(img)\n",
    "                res_refine = res.pandas().xyxy[0].values\n",
    "                nms_human = len(res_refine)\n",
    "                if nms_human > 0:\n",
    "                    for bbox in res_refine:\n",
    "                        \"\"\"바운딩 박스 상하좌우 크기 조절\"\"\"\n",
    "                        xx1, yy1, xx2, yy2 = int(bbox[0])-10, int(bbox[1]), int(bbox[2])+10, int(bbox[3])\n",
    "                        if xx1 < 0:\n",
    "                            xx1 = 0\n",
    "                        elif xx2 > 639:\n",
    "                            xx2 = 639\n",
    "                        if yy1 < 0:\n",
    "                            yy1 = 0\n",
    "                        elif yy2 > 639:\n",
    "                            yy2 = 639\n",
    "\n",
    "                        start_point = (xx1, yy1)\n",
    "                        end_point = (xx2, yy2)\n",
    "\n",
    "                        \"\"\" Yolov5 바운딩 박스 좌표 안에서 mediapipe Pose 추출\"\"\"\n",
    "                        if bbox[4] > y_CONFIDENCE: # bbox[4] : confidence 데이터\n",
    "                            # img = cv2.rectangle(img, start_point, end_point, (0, 0, 255), 2) # 바운딩 박스 그리기 : 데이터 추출 확인용\n",
    "                            c_img = img[yy1:yy2, xx1:xx2] # 바운딩 박스 좌표\n",
    "                            results = pose.process(cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)) # Yolov5 바운딩 박스 좌표 안에서 'mp_pose' 좌표\n",
    "\n",
    "                            if not results.pose_landmarks: continue\n",
    "                            idx = 0\n",
    "                            draw_line_dic = {}\n",
    "                            xy_list, xy_list_flip = [], []\n",
    "                            # 33 반복문 진행 : 33개 중 18개의 dot\n",
    "                            for x_and_y in results.pose_landmarks.landmark:\n",
    "                                if idx in attention_dot:\n",
    "                                    xy_list.append(x_and_y.x)\n",
    "                                    xy_list.append(x_and_y.y)\n",
    "                                    xy_list_flip.append(1 - x_and_y.x)\n",
    "                                    xy_list_flip.append(x_and_y.y)\n",
    "\n",
    "                                    x, y = int(x_and_y.x*(xx2-xx1)), int(x_and_y.y*(yy2-yy1))\n",
    "                                    draw_line_dic[idx] = [x, y]\n",
    "                                idx += 1\n",
    "\n",
    "                            if len(xy_list) != len(attention_dot) * 2:\n",
    "                                print('Error : attention_dot 데이터 오류')\n",
    "\n",
    "                            xy_list_list.append(xy_list)\n",
    "                            xy_list_list_flip.append(xy_list_flip)\n",
    "\n",
    "                            \"\"\"mediapipe line 그리기 부분 : 데이터 추출(dot) 확인용\"\"\"\n",
    "                            # for line in draw_line:\n",
    "                            #     x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
    "                            #     x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
    "                            #     c_img = cv2.line(c_img, (x1, y1), (x2, y2), (0, 255, 0), 4)\n",
    "                            # # cv2.imshow('Landmark Image', img)\n",
    "                            # cv2_imshow(img)\n",
    "                            # cv2.waitKey(1)\n",
    "\n",
    "            elif ret == False: break\n",
    "\n",
    "\n",
    "        # 부족한 프레임 수 맞추기\n",
    "        if len(xy_list_list_flip) < 15:\n",
    "            return False, False\n",
    "        elif len(xy_list_list_flip) < frame_length:\n",
    "            f_ln = frame_length - len(xy_list_list_flip)\n",
    "            for _ in range(f_ln):\n",
    "                xy_list_list.append(xy_list_list[-1])\n",
    "                xy_list_list_flip.append(xy_list_list_flip[-1])\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    return xy_list_list, xy_list_list_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../dataset_aihub/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path = '../../dataset_aihub/' # dataset 경로\n",
    "video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal_강동욱', 'normal_강동욱', '.DS_Store', 'abnormal_김성욱', 'normal_김성욱', 'normal_강민지', 'abnormal_김성욱.pickle', 'normal_강민지.pickle', 'abnormal_강민지', 'normal_김성욱.pickle', 'abnormal_강민지.pickle']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(video_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 데이터에서 mp pose landmark dot 데이터 추출 부분\n",
    "raw_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in os.listdir(video_path):\n",
    "#    for video_name in os.listdir(video_path + '/' + fold):\n",
    "#        if int(video_name.split('_')[2][:2]) >= 30: # video name 참조\n",
    "#            if video_name.split('_')[1] == 'normal': label = 0\n",
    "#            else: label = 1\n",
    "#            skel_data_n, skel_data_f = get_skeleton('{}/{}'.format(video_path + '/' + fold, video_name), attention_dot, draw_line)\n",
    "#            if skel_data_n != False:\n",
    "#                seq_list_n = skel_data_n[:30]\n",
    "#                seq_list_f = skel_data_f[:30]\n",
    "#                raw_data.append({'key':label, 'value':seq_list_n})\n",
    "#                raw_data.append({'key':label, 'value':seq_list_f})\n",
    "# random.shuffle(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 'abnormal_김성욱'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713925506.950551 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1713925511.373322 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925514.912259 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925518.465557 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925522.637580 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925526.109154 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925529.713653 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925533.949761 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925537.484679 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925542.705914 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925547.131919 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925551.414632 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925555.597268 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925560.720610 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925565.870275 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925569.527002 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925573.185431 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925578.293196 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925581.366903 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925586.310618 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925589.848099 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925593.431222 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925597.010069 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925602.492777 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925606.667472 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925610.834648 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925614.656572 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925619.452762 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925625.144330 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925629.627796 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925634.507692 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925641.619345 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925652.324088 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713925659.898431 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925667.040029 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925673.892237 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925683.256754 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925691.548729 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925699.659104 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925707.416840 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925718.530044 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925723.749148 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925730.664721 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925736.334567 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925744.698111 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925751.364103 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925759.183410 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925767.105436 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925772.424203 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713925779.299708 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925788.510169 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925794.229668 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925802.508413 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925808.982230 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925814.817535 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925820.852718 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925828.649119 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925835.817864 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925843.491659 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925849.292086 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925857.511664 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925864.412607 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925871.344407 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925878.041367 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925883.811235 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925892.092883 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713925899.606046 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925906.262769 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925917.100696 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925922.363149 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925929.084725 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925936.589684 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925944.399504 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713925951.494336 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n"
     ]
    }
   ],
   "source": [
    "for video_name in os.listdir(video_path + '/' + fold):\n",
    "    if int(video_name.split('_')[2][:2]) >= 30: # video name 참조\n",
    "        if video_name.split('_')[1] == 'normal': label = 0\n",
    "        else: label = 1\n",
    "        skel_data_n, skel_data_f = get_skeleton('{}/{}'.format(video_path + '/' + fold, video_name), attention_dot, draw_line)\n",
    "        if skel_data_n != False:\n",
    "            seq_list_n = skel_data_n[:30]\n",
    "            seq_list_f = skel_data_f[:30]\n",
    "            raw_data.append({'key':label, 'value':seq_list_n})\n",
    "            raw_data.append({'key':label, 'value':seq_list_f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../dataset_aihub/abnormal_김성욱.pickle\n"
     ]
    }
   ],
   "source": [
    "filename = video_path + fold + '.pickle'\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(raw_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 'normal_김성욱'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1713923467.303726 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923472.308378 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923475.638770 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923478.803307 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923482.759004 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923487.288174 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923490.541359 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923494.509557 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923498.627024 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923502.357009 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923506.563674 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923511.406712 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923515.347779 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713923520.691875 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923524.207789 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923528.233902 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923531.968615 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923536.938438 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923540.980089 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923545.140296 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923549.739591 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923553.507858 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923557.891020 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923562.104338 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923567.079354 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923571.579189 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923577.704923 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923582.088042 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923587.447530 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923594.693370 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713923599.996015 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923606.121909 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923613.720444 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923620.619171 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923628.876491 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923635.020109 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923644.930563 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923652.359343 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923658.473437 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923667.012678 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923673.017907 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923678.854370 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923686.334733 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923693.655102 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923699.693022 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923705.823675 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923712.523804 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713923718.828797 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923723.064631 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923728.983807 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923733.912241 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923738.666899 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923743.885250 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923749.246802 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923755.612519 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923761.272472 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923767.147829 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923774.587663 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923780.332998 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923786.247202 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923791.261117 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923799.371226 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923806.562856 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713923811.276714 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923815.495489 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923822.548533 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923828.907405 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923833.476732 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923839.265316 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923845.498376 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923854.339594 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923859.658668 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923865.087410 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713923869.890024 2117701 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n"
     ]
    }
   ],
   "source": [
    "for video_name in os.listdir(video_path + '/' + fold):\n",
    "    if int(video_name.split('_')[2][:2]) >= 30: # video name 참조\n",
    "        if video_name.split('_')[1] == 'normal': label = 0\n",
    "        else: label = 1\n",
    "        skel_data_n, skel_data_f = get_skeleton('{}/{}'.format(video_path + '/' + fold, video_name), attention_dot, draw_line)\n",
    "        if skel_data_n != False:\n",
    "            seq_list_n = skel_data_n[:30]\n",
    "            seq_list_f = skel_data_f[:30]\n",
    "            raw_data.append({'key':label, 'value':seq_list_n})\n",
    "            raw_data.append({'key':label, 'value':seq_list_f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../dataset_aihub/normal_김성욱.pickle\n"
     ]
    }
   ],
   "source": [
    "filename = video_path + fold + '.pickle'\n",
    "print(filename)\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(raw_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 'normal_강민지'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1713926004.397779 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926008.272915 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926011.759046 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926015.482457 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926019.153446 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926023.855109 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926026.943515 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926030.758632 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926034.335859 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713926037.962627 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926042.368035 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926047.899196 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926052.716701 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926057.230190 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926062.277451 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926068.665618 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926075.379329 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926081.780968 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926088.474370 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926095.348077 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926102.397928 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926108.679812 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926115.698309 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926122.433335 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926128.835415 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713926136.023887 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926142.560822 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926149.612347 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926156.022480 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926163.299996 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926169.560926 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926175.793766 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926181.850747 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926188.021821 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926194.008601 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926199.769154 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926206.031532 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926212.568275 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926219.187892 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926225.468546 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926231.929946 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926238.312049 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713926246.537993 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926252.129672 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926260.595342 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926266.307696 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926272.428371 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926279.260490 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926286.093448 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926292.801277 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926299.442175 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926306.457415 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926312.767041 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926319.022784 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926325.240277 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926331.546522 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926337.583314 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n"
     ]
    }
   ],
   "source": [
    "for video_name in os.listdir(video_path + '/' + fold):\n",
    "    if int(video_name.split('_')[2][:2]) >= 30: # video name 참조\n",
    "        if video_name.split('_')[1] == 'normal': label = 0\n",
    "        else: label = 1\n",
    "        skel_data_n, skel_data_f = get_skeleton('{}/{}'.format(video_path + '/' + fold, video_name), attention_dot, draw_line)\n",
    "        if skel_data_n != False:\n",
    "            seq_list_n = skel_data_n[:30]\n",
    "            seq_list_f = skel_data_f[:30]\n",
    "            raw_data.append({'key':label, 'value':seq_list_n})\n",
    "            raw_data.append({'key':label, 'value':seq_list_f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../dataset_aihub/normal_강민지.pickle\n"
     ]
    }
   ],
   "source": [
    "filename = video_path + fold + '.pickle'\n",
    "print(filename)\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(raw_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 'abnormal_강민지'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1713926420.539487 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926424.109958 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713926427.611580 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926431.016424 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926434.419624 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926437.867506 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926441.324933 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926444.885199 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926448.132161 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926451.639177 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926455.226392 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926459.603685 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926463.150704 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926466.756428 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926470.702122 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926474.081338 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926478.069593 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926482.020930 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713926485.976853 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926491.195071 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926495.724335 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926500.332566 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926505.264266 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926510.705772 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926519.287175 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926526.863637 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926534.509887 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926542.644850 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926550.677405 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926558.429940 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926566.471219 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926573.879980 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926580.860914 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926587.826679 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926594.220598 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713926600.310012 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926606.595391 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926612.705000 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926619.018583 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926625.067798 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926632.543356 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926637.058526 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926642.988949 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926648.262973 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926654.118545 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926659.789494 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926665.435938 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926670.957793 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926676.164548 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926681.786255 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926687.414997 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926693.034892 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "Context leak detected, msgtracer returned -1\n",
      "I0000 00:00:1713926697.844261 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926703.570304 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926709.251557 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926715.297556 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926721.096027 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926727.063726 2150928 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n"
     ]
    }
   ],
   "source": [
    "for video_name in os.listdir(video_path + '/' + fold):\n",
    "    if int(video_name.split('_')[2][:2]) >= 30: # video name 참조\n",
    "        if video_name.split('_')[1] == 'normal': label = 0\n",
    "        else: label = 1\n",
    "        skel_data_n, skel_data_f = get_skeleton('{}/{}'.format(video_path + '/' + fold, video_name), attention_dot, draw_line)\n",
    "        if skel_data_n != False:\n",
    "            seq_list_n = skel_data_n[:30]\n",
    "            seq_list_f = skel_data_f[:30]\n",
    "            raw_data.append({'key':label, 'value':seq_list_n})\n",
    "            raw_data.append({'key':label, 'value':seq_list_f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../dataset_aihub/abnormal_강민지.pickle\n"
     ]
    }
   ],
   "source": [
    "filename = video_path + fold + '.pickle'\n",
    "print(filename)\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(raw_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 'normal_강동욱'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1713926964.097229 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926968.456876 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926971.895533 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926974.960526 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926978.443213 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926982.205812 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926986.383519 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926990.447323 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926994.166049 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713926997.865636 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713927001.440181 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713927005.203698 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713927008.644144 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n",
      "I0000 00:00:1713927013.011167 2165579 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(video_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m fold):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(video_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m][:\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m: \u001b[38;5;66;03m# video name 참조\u001b[39;00m\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m video_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m: label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m: label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for video_name in os.listdir(video_path + '/' + fold):\n",
    "    if int(video_name.split('_')[2][:2]) >= 30: # video name 참조\n",
    "        if video_name.split('_')[1] == 'normal': label = 0\n",
    "        else: label = 1\n",
    "        skel_data_n, skel_data_f = get_skeleton('{}/{}'.format(video_path + '/' + fold, video_name), attention_dot, draw_line)\n",
    "        if skel_data_n != False:\n",
    "            seq_list_n = skel_data_n[:30]\n",
    "            seq_list_f = skel_data_f[:30]\n",
    "            raw_data.append({'key':label, 'value':seq_list_n})\n",
    "            raw_data.append({'key':label, 'value':seq_list_f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = video_path + fold + '.pickle'\n",
    "print(filename)\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(raw_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 'abnormal_강동욱'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_name in os.listdir(video_path + '/' + fold):\n",
    "    if int(video_name.split('_')[2][:2]) >= 30: # video name 참조\n",
    "        if video_name.split('_')[1] == 'normal': label = 0\n",
    "        else: label = 1\n",
    "        skel_data_n, skel_data_f = get_skeleton('{}/{}'.format(video_path + '/' + fold, video_name), attention_dot, draw_line)\n",
    "        if skel_data_n != False:\n",
    "            seq_list_n = skel_data_n[:30]\n",
    "            seq_list_f = skel_data_f[:30]\n",
    "            raw_data.append({'key':label, 'value':seq_list_n})\n",
    "            raw_data.append({'key':label, 'value':seq_list_f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = video_path + fold + '.pickle'\n",
    "print(filename)\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(raw_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"abnormal_김성욱.pickle\", 'rb') as f:\n",
    "    data1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "pickles = sorted(glob.glob('../../dataset_aihub/*.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []\n",
    "for pickle in pickles:\n",
    "    with open(pickle, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    raw_data += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal data: 14 | abnormal data: 16\n"
     ]
    }
   ],
   "source": [
    "# dataset 길이 출력\n",
    "\n",
    "nd = 0\n",
    "ad = 0\n",
    "for i in range(len(raw_data)):\n",
    "    if raw_data[i]['key'] == 0:\n",
    "        nd += 1\n",
    "    else:\n",
    "        ad += 1\n",
    "print('normal data:', nd, '| abnormal data:', ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, seq_list):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        for dic in seq_list :\n",
    "            self.y.append(dic['key'])\n",
    "            self.X.append(dic['value'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.X[index]\n",
    "        label = self.y[index]\n",
    "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24, 3, 3\n"
     ]
    }
   ],
   "source": [
    "split_ratio = [0.8, 0.1, 0.1]\n",
    "train_len = int(len(raw_data) * split_ratio[0])\n",
    "val_len = int(len(raw_data) * split_ratio[1])\n",
    "test_len = len(raw_data) - train_len - val_len\n",
    "\n",
    "print('{}, {}, {}'.format(train_len, val_len, test_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(raw_data)\n",
    "train_data, valid_data, test_data = random_split(train_dataset, [train_len, val_len, test_len])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "class skeleton_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(skeleton_LSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=len(attention_dot) * 2, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm5 = nn.LSTM(input_size=256, hidden_size=128, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=NUM_LAYERS, batch_first=True)\n",
    "        self.fc = nn.Linear(32,2)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x, _ = self.lstm5(x)\n",
    "        x, _ = self.lstm6(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm7(x)\n",
    "        x = self.fc(x[:,-1,:]) # x[배치 크기, 시퀀스 길이, 은닉 상태 크기], [:, -1, :] -> 마지막 시간 단계만 선택\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "\n",
    "def init_model():\n",
    "    global net, loss_fn, optim\n",
    "    plt.rc('font', size = 10)\n",
    "    net = skeleton_LSTM().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optim = Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# epoch 카운터 초기화\n",
    "def init_epoch():\n",
    "    global epoch_cnt\n",
    "    epoch_cnt = 0\n",
    "\n",
    "# 모든 Log를 초기화\n",
    "def init_log():\n",
    "    global log_stack, iter_log, tloss_log, tacc_log, vloss_log, vacc_log, time_log\n",
    "    plt.rc('font', size = 10)\n",
    "    iter_log, tloss_log, tacc_log, vloss_log, vacc_log = [], [], [], [], []\n",
    "    time_log, log_stack = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_train_log(_tloss, _tacc, _time):\n",
    "    # Train Log 기록\n",
    "    time_log.append(_time)\n",
    "    tloss_log.append(_tloss)\n",
    "    tacc_log.append(_tacc)\n",
    "    iter_log.append(epoch_cnt)\n",
    "\n",
    "def record_valid_log(_vloss, _vacc):\n",
    "    # Validation Log 기록\n",
    "    vloss_log.append(_vloss)\n",
    "    vacc_log.append(_vacc)\n",
    "\n",
    "def last(log_list):\n",
    "    # last 안의 마지막 숫자를 반환(print_log 함수에서 사용)\n",
    "    if len(log_list) > 0:\n",
    "        return log_list[len(log_list) - 1]\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def print_log():\n",
    "    # 학습 추이 출력 : 소숫점 3자리까지\n",
    "    train_loss = round(float(last(tloss_log)), 3)\n",
    "    train_acc = round(float(last(tacc_log)), 3)\n",
    "    val_loss = round(float(last(vloss_log)), 3)\n",
    "    val_acc = round(float(last(vacc_log)), 3)\n",
    "    time_spent = round(float(last(time_log)), 3)\n",
    "\n",
    "    log_str = 'Epoch: {:3} | T_Loss {:5} | T_Acc {:5} | V_Loss {:5} | V_Acc {:5} | {:5}'.format(last(iter_log), train_loss, train_acc, val_loss, val_acc, time_spent)\n",
    "\n",
    "    log_stack.append(log_str)\n",
    "    \n",
    "    # 학습 추이 그래프 출력\n",
    "    hist_fig, loss_axis = plt.subplots(figsize=(10, 3), dpi=99)\n",
    "    hist_fig.patch.set_facecolor('white')\n",
    "\n",
    "    # Loss Line 구성\n",
    "    loss_t_line = plt.plot(iter_log, tloss_log, label='Train_Loss', color='red', marker='o')\n",
    "    loss_v_line = plt.plot(iter_log, vloss_log, label='Valid_Loss', color='blue', marker='s')\n",
    "    loss_axis.set_xlabel('epoch')\n",
    "    loss_axis.set_ylabel('loss')\n",
    "\n",
    "    # Acc, Line 구성\n",
    "    acc_axis = loss_axis.twinx()\n",
    "    acc_t_line = acc_axis.plot(iter_log, tacc_log, label='Train_Acc', color='red', marker='+')\n",
    "    acc_v_line = acc_axis.plot(iter_log, vacc_log, label='Valid_Acc', color='blue', marker='x')\n",
    "    acc_axis.set_ylabel('accuracy')\n",
    "\n",
    "    # 그래프 출력\n",
    "    hist_lines = loss_t_line + loss_v_line + acc_t_line + acc_v_line\n",
    "    loss_axis.legend(hist_lines, [l.get_label() for l in hist_lines])\n",
    "    loss_axis.grid()\n",
    "    plt.title('Learning history until epoch {}'.format(last(iter_log)))\n",
    "    plt.draw()\n",
    "\n",
    "    # 텍스트 로그 출력\n",
    "    clear_output(wait=True)\n",
    "    plt.show()\n",
    "    for idx in reversed(range(len(log_stack))):\n",
    "        print(log_stack[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    if device != 'cpu':\n",
    "        empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# 학습 알고리즘\n",
    "def epoch(data_loader, mode = 'train'):\n",
    "    global epoch_cnt\n",
    "\n",
    "    # 사용되는 변수 초기화\n",
    "    iter_loss, iter_acc, last_grad_performed = [], [], False\n",
    "\n",
    "    # 1 iteration 학습 알고리즘(for문을 나오면 1 epoch 완료)\n",
    "    for _data, _label in data_loader:\n",
    "        data, label = _data.to(device), _label.type(torch.LongTensor).to(device)\n",
    "\n",
    "        # 1. Feed-forward\n",
    "        if mode == 'train':\n",
    "            net.train()\n",
    "        else:\n",
    "            # 학습때만 쓰이는 Dropout, Batch Mormalization을 미사용\n",
    "            net.eval()\n",
    "\n",
    "        result = net(data) # 1 Batch에 대한 결과가 모든 Class에 대한 확률값으로\n",
    "        _, out = torch.max(result, 1) # result에서 최대 확률값을 기준으로 예측 class 도출( _ : 값 부분은 필요 없음, out : index 중 가장 큰 하나의 데이터)\n",
    "\n",
    "        # 2. Loss 계산\n",
    "        loss = loss_fn(result, label) # GT 와 Label 비교하여 Loss 산정\n",
    "        iter_loss.append(loss.item()) # 학습 추이를 위하여 Loss를 기록\n",
    "\n",
    "        # 3. 역전파 학습 후 Gradient Descent\n",
    "        if mode == 'train':\n",
    "            optim.zero_grad() # 미분을 통해 얻은 기울기를 초기화 for 다음 epoch\n",
    "            loss.backward() # 역전파 학습\n",
    "            optim.step() # Gradient Descent 수행\n",
    "            last_grad_performed = True # for문을 나가면 epoch 카운터 += 1\n",
    "\n",
    "        # 4. 정확도 계산\n",
    "        acc_partial = (out == label).float().sum() # GT == Label 인 개수\n",
    "        acc_partial = acc_partial / len(label) # ( TP / (TP + TM)) 해서 정확도 산출\n",
    "        iter_acc.append(acc_partial.item()) # 학습 추이를 위하여 Acc. 기록\n",
    "\n",
    "    # 역전파 학습 후 Epoch 카운터 += 1\n",
    "    if last_grad_performed:\n",
    "        epoch_cnt += 1\n",
    "\n",
    "    clear_memory()\n",
    "\n",
    "    # loss와 acc의 평균값 for 학습추이 그래프, 모든 GT와 Label 값 for 컨퓨전 매트릭스\n",
    "    return np.average(iter_loss), np.average(iter_acc)\n",
    "\n",
    "def epoch_not_finished():\n",
    "    # 에폭이 끝남을 알림\n",
    "    return epoch_cnt < maximum_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 700 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.511\n",
      "Epoch: 699 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.504\n",
      "Epoch: 698 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.507\n",
      "Epoch: 697 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.506\n",
      "Epoch: 696 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.501\n",
      "Epoch: 695 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 |   1.5\n",
      "Epoch: 694 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.495\n",
      "Epoch: 693 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.494\n",
      "Epoch: 692 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.491\n",
      "Epoch: 691 | T_Loss 0.694 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.489\n",
      "Epoch: 690 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.493\n",
      "Epoch: 689 | T_Loss 0.695 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.488\n",
      "Epoch: 688 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.488\n",
      "Epoch: 687 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.487\n",
      "Epoch: 686 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.484\n",
      "Epoch: 685 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.483\n",
      "Epoch: 684 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 |  1.48\n",
      "Epoch: 683 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.476\n",
      "Epoch: 682 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.474\n",
      "Epoch: 681 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.475\n",
      "Epoch: 680 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.469\n",
      "Epoch: 679 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 |  1.47\n",
      "Epoch: 678 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.468\n",
      "Epoch: 677 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.468\n",
      "Epoch: 676 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.461\n",
      "Epoch: 675 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.461\n",
      "Epoch: 674 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.457\n",
      "Epoch: 673 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.454\n",
      "Epoch: 672 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.456\n",
      "Epoch: 671 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.451\n",
      "Epoch: 670 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.453\n",
      "Epoch: 669 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 |  1.45\n",
      "Epoch: 668 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.449\n",
      "Epoch: 667 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.445\n",
      "Epoch: 666 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.447\n",
      "Epoch: 665 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.442\n",
      "Epoch: 664 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.442\n",
      "Epoch: 663 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.441\n",
      "Epoch: 662 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.436\n",
      "Epoch: 661 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.437\n",
      "Epoch: 660 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.435\n",
      "Epoch: 659 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.431\n",
      "Epoch: 658 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.429\n",
      "Epoch: 657 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |  1.43\n",
      "Epoch: 656 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.425\n",
      "Epoch: 655 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.427\n",
      "Epoch: 654 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |  1.42\n",
      "Epoch: 653 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.422\n",
      "Epoch: 652 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.417\n",
      "Epoch: 651 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.418\n",
      "Epoch: 650 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.415\n",
      "Epoch: 649 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.414\n",
      "Epoch: 648 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.413\n",
      "Epoch: 647 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.406\n",
      "Epoch: 646 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 |  1.41\n",
      "Epoch: 645 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.407\n",
      "Epoch: 644 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.406\n",
      "Epoch: 643 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.402\n",
      "Epoch: 642 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 |   1.4\n",
      "Epoch: 641 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 1.401\n",
      "Epoch: 640 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 1.401\n",
      "Epoch: 639 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 1.396\n",
      "Epoch: 638 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 1.392\n",
      "Epoch: 637 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 1.389\n",
      "Epoch: 636 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 1.389\n",
      "Epoch: 635 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 |  1.39\n",
      "Epoch: 634 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 1.386\n",
      "Epoch: 633 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 1.384\n",
      "Epoch: 632 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.383\n",
      "Epoch: 631 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.382\n",
      "Epoch: 630 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.378\n",
      "Epoch: 629 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.376\n",
      "Epoch: 628 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.375\n",
      "Epoch: 627 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.372\n",
      "Epoch: 626 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 |  1.37\n",
      "Epoch: 625 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 |  1.37\n",
      "Epoch: 624 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.368\n",
      "Epoch: 623 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.368\n",
      "Epoch: 622 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.366\n",
      "Epoch: 621 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.363\n",
      "Epoch: 620 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.361\n",
      "Epoch: 619 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.358\n",
      "Epoch: 618 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.359\n",
      "Epoch: 617 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.356\n",
      "Epoch: 616 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.352\n",
      "Epoch: 615 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.352\n",
      "Epoch: 614 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.349\n",
      "Epoch: 613 | T_Loss 0.694 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.349\n",
      "Epoch: 612 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.345\n",
      "Epoch: 611 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.343\n",
      "Epoch: 610 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.346\n",
      "Epoch: 609 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.339\n",
      "Epoch: 608 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 |  1.34\n",
      "Epoch: 607 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.336\n",
      "Epoch: 606 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.332\n",
      "Epoch: 605 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.332\n",
      "Epoch: 604 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.328\n",
      "Epoch: 603 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.326\n",
      "Epoch: 602 | T_Loss 0.694 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.327\n",
      "Epoch: 601 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.324\n",
      "Epoch: 600 | T_Loss 0.694 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.324\n",
      "Epoch: 599 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.321\n",
      "Epoch: 598 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 |  1.32\n",
      "Epoch: 597 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.315\n",
      "Epoch: 596 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.316\n",
      "Epoch: 595 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.314\n",
      "Epoch: 594 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.316\n",
      "Epoch: 593 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.311\n",
      "Epoch: 592 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.309\n",
      "Epoch: 591 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.305\n",
      "Epoch: 590 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.305\n",
      "Epoch: 589 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.304\n",
      "Epoch: 588 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.302\n",
      "Epoch: 587 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.301\n",
      "Epoch: 586 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.298\n",
      "Epoch: 585 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.298\n",
      "Epoch: 584 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.292\n",
      "Epoch: 583 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.294\n",
      "Epoch: 582 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.288\n",
      "Epoch: 581 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 |  1.29\n",
      "Epoch: 580 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.287\n",
      "Epoch: 579 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.286\n",
      "Epoch: 578 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.282\n",
      "Epoch: 577 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.281\n",
      "Epoch: 576 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.279\n",
      "Epoch: 575 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.278\n",
      "Epoch: 574 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.272\n",
      "Epoch: 573 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.271\n",
      "Epoch: 572 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.274\n",
      "Epoch: 571 | T_Loss 0.695 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.269\n",
      "Epoch: 570 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.267\n",
      "Epoch: 569 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.269\n",
      "Epoch: 568 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.265\n",
      "Epoch: 567 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.265\n",
      "Epoch: 566 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 |  1.26\n",
      "Epoch: 565 | T_Loss 0.696 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 |  1.26\n",
      "Epoch: 564 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.258\n",
      "Epoch: 563 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.257\n",
      "Epoch: 562 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.257\n",
      "Epoch: 561 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.253\n",
      "Epoch: 560 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.252\n",
      "Epoch: 559 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.251\n",
      "Epoch: 558 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.247\n",
      "Epoch: 557 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.245\n",
      "Epoch: 556 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.244\n",
      "Epoch: 555 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.243\n",
      "Epoch: 554 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.238\n",
      "Epoch: 553 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.238\n",
      "Epoch: 552 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.237\n",
      "Epoch: 551 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.236\n",
      "Epoch: 550 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.233\n",
      "Epoch: 549 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.231\n",
      "Epoch: 548 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.228\n",
      "Epoch: 547 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.223\n",
      "Epoch: 546 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.226\n",
      "Epoch: 545 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.225\n",
      "Epoch: 544 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.225\n",
      "Epoch: 543 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.222\n",
      "Epoch: 542 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.218\n",
      "Epoch: 541 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.214\n",
      "Epoch: 540 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.214\n",
      "Epoch: 539 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 1.213\n",
      "Epoch: 538 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.212\n",
      "Epoch: 537 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.209\n",
      "Epoch: 536 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 |  1.21\n",
      "Epoch: 535 | T_Loss 0.695 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.207\n",
      "Epoch: 534 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.206\n",
      "Epoch: 533 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 |   1.2\n",
      "Epoch: 532 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.201\n",
      "Epoch: 531 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.199\n",
      "Epoch: 530 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.194\n",
      "Epoch: 529 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.195\n",
      "Epoch: 528 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.188\n",
      "Epoch: 527 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.194\n",
      "Epoch: 526 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.189\n",
      "Epoch: 525 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.189\n",
      "Epoch: 524 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.187\n",
      "Epoch: 523 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.182\n",
      "Epoch: 522 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.181\n",
      "Epoch: 521 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.181\n",
      "Epoch: 520 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.178\n",
      "Epoch: 519 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 1.178\n",
      "Epoch: 518 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.177\n",
      "Epoch: 517 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.172\n",
      "Epoch: 516 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.169\n",
      "Epoch: 515 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.169\n",
      "Epoch: 514 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.169\n",
      "Epoch: 513 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.166\n",
      "Epoch: 512 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.162\n",
      "Epoch: 511 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.166\n",
      "Epoch: 510 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.161\n",
      "Epoch: 509 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |  1.16\n",
      "Epoch: 508 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.155\n",
      "Epoch: 507 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.155\n",
      "Epoch: 506 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.152\n",
      "Epoch: 505 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.152\n",
      "Epoch: 504 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |  1.15\n",
      "Epoch: 503 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.148\n",
      "Epoch: 502 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.145\n",
      "Epoch: 501 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.147\n",
      "Epoch: 500 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.144\n",
      "Epoch: 499 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 |  1.14\n",
      "Epoch: 498 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.141\n",
      "Epoch: 497 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.139\n",
      "Epoch: 496 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.132\n",
      "Epoch: 495 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.135\n",
      "Epoch: 494 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.132\n",
      "Epoch: 493 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.129\n",
      "Epoch: 492 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.129\n",
      "Epoch: 491 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.127\n",
      "Epoch: 490 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.124\n",
      "Epoch: 489 | T_Loss  0.69 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 |  1.12\n",
      "Epoch: 488 | T_Loss 0.691 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.118\n",
      "Epoch: 487 | T_Loss 0.692 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.119\n",
      "Epoch: 486 | T_Loss 0.688 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.118\n",
      "Epoch: 485 | T_Loss 0.688 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.116\n",
      "Epoch: 484 | T_Loss 0.688 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.114\n",
      "Epoch: 483 | T_Loss 0.691 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.111\n",
      "Epoch: 482 | T_Loss  0.69 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.112\n",
      "Epoch: 481 | T_Loss 0.688 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.107\n",
      "Epoch: 480 | T_Loss  0.69 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.106\n",
      "Epoch: 479 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.104\n",
      "Epoch: 478 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.104\n",
      "Epoch: 477 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.101\n",
      "Epoch: 476 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.102\n",
      "Epoch: 475 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.098\n",
      "Epoch: 474 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.099\n",
      "Epoch: 473 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.096\n",
      "Epoch: 472 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.094\n",
      "Epoch: 471 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.093\n",
      "Epoch: 470 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 |  1.09\n",
      "Epoch: 469 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.089\n",
      "Epoch: 468 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.086\n",
      "Epoch: 467 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.082\n",
      "Epoch: 466 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.084\n",
      "Epoch: 465 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.082\n",
      "Epoch: 464 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.078\n",
      "Epoch: 463 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.079\n",
      "Epoch: 462 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.075\n",
      "Epoch: 461 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.073\n",
      "Epoch: 460 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.069\n",
      "Epoch: 459 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |  1.07\n",
      "Epoch: 458 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.067\n",
      "Epoch: 457 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.066\n",
      "Epoch: 456 | T_Loss 0.694 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.064\n",
      "Epoch: 455 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.063\n",
      "Epoch: 454 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.063\n",
      "Epoch: 453 | T_Loss 0.694 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.061\n",
      "Epoch: 452 | T_Loss 0.689 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.056\n",
      "Epoch: 451 | T_Loss 0.689 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.059\n",
      "Epoch: 450 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 1.058\n",
      "Epoch: 449 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 1.054\n",
      "Epoch: 448 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 1.051\n",
      "Epoch: 447 | T_Loss 0.688 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.049\n",
      "Epoch: 446 | T_Loss 0.687 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 1.046\n",
      "Epoch: 445 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.047\n",
      "Epoch: 444 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.042\n",
      "Epoch: 443 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 |  1.04\n",
      "Epoch: 442 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.037\n",
      "Epoch: 441 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.036\n",
      "Epoch: 440 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.035\n",
      "Epoch: 439 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.035\n",
      "Epoch: 438 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |  1.03\n",
      "Epoch: 437 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.031\n",
      "Epoch: 436 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.026\n",
      "Epoch: 435 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.026\n",
      "Epoch: 434 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.023\n",
      "Epoch: 433 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.025\n",
      "Epoch: 432 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.022\n",
      "Epoch: 431 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 |  1.02\n",
      "Epoch: 430 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.016\n",
      "Epoch: 429 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.018\n",
      "Epoch: 428 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 1.016\n",
      "Epoch: 427 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.011\n",
      "Epoch: 426 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 1.018\n",
      "Epoch: 425 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.009\n",
      "Epoch: 424 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.005\n",
      "Epoch: 423 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.007\n",
      "Epoch: 422 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 1.004\n",
      "Epoch: 421 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 1.001\n",
      "Epoch: 420 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 |   1.0\n",
      "Epoch: 419 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.999\n",
      "Epoch: 418 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |   1.0\n",
      "Epoch: 417 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.993\n",
      "Epoch: 416 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.993\n",
      "Epoch: 415 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.992\n",
      "Epoch: 414 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.987\n",
      "Epoch: 413 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |  0.99\n",
      "Epoch: 412 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.987\n",
      "Epoch: 411 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.985\n",
      "Epoch: 410 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.982\n",
      "Epoch: 409 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.979\n",
      "Epoch: 408 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.979\n",
      "Epoch: 407 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.979\n",
      "Epoch: 406 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.978\n",
      "Epoch: 405 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.977\n",
      "Epoch: 404 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.973\n",
      "Epoch: 403 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.971\n",
      "Epoch: 402 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.969\n",
      "Epoch: 401 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.967\n",
      "Epoch: 400 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.962\n",
      "Epoch: 399 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 |  0.96\n",
      "Epoch: 398 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.961\n",
      "Epoch: 397 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.959\n",
      "Epoch: 396 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.956\n",
      "Epoch: 395 | T_Loss 0.695 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.958\n",
      "Epoch: 394 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.951\n",
      "Epoch: 393 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 |  0.95\n",
      "Epoch: 392 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.947\n",
      "Epoch: 391 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 |  0.95\n",
      "Epoch: 390 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.944\n",
      "Epoch: 389 | T_Loss 0.696 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.941\n",
      "Epoch: 388 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.942\n",
      "Epoch: 387 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |  0.94\n",
      "Epoch: 386 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.939\n",
      "Epoch: 385 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.935\n",
      "Epoch: 384 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.935\n",
      "Epoch: 383 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.931\n",
      "Epoch: 382 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.929\n",
      "Epoch: 381 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.928\n",
      "Epoch: 380 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.927\n",
      "Epoch: 379 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.923\n",
      "Epoch: 378 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.925\n",
      "Epoch: 377 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |  0.92\n",
      "Epoch: 376 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.917\n",
      "Epoch: 375 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.919\n",
      "Epoch: 374 | T_Loss 0.685 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.913\n",
      "Epoch: 373 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.911\n",
      "Epoch: 372 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.913\n",
      "Epoch: 371 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.908\n",
      "Epoch: 370 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.909\n",
      "Epoch: 369 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.904\n",
      "Epoch: 368 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.904\n",
      "Epoch: 367 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.902\n",
      "Epoch: 366 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |   0.9\n",
      "Epoch: 365 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.897\n",
      "Epoch: 364 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.898\n",
      "Epoch: 363 | T_Loss 0.685 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.892\n",
      "Epoch: 362 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.893\n",
      "Epoch: 361 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.892\n",
      "Epoch: 360 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.892\n",
      "Epoch: 359 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.888\n",
      "Epoch: 358 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.886\n",
      "Epoch: 357 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.883\n",
      "Epoch: 356 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.881\n",
      "Epoch: 355 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 |  0.88\n",
      "Epoch: 354 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.875\n",
      "Epoch: 353 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.874\n",
      "Epoch: 352 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.873\n",
      "Epoch: 351 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.871\n",
      "Epoch: 350 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.872\n",
      "Epoch: 349 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.868\n",
      "Epoch: 348 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.867\n",
      "Epoch: 347 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.864\n",
      "Epoch: 346 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.864\n",
      "Epoch: 345 | T_Loss 0.684 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.864\n",
      "Epoch: 344 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.859\n",
      "Epoch: 343 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.858\n",
      "Epoch: 342 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.857\n",
      "Epoch: 341 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.853\n",
      "Epoch: 340 | T_Loss  0.69 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 0.854\n",
      "Epoch: 339 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 |  0.85\n",
      "Epoch: 338 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.846\n",
      "Epoch: 337 | T_Loss 0.694 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.848\n",
      "Epoch: 336 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.847\n",
      "Epoch: 335 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.845\n",
      "Epoch: 334 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.842\n",
      "Epoch: 333 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.839\n",
      "Epoch: 332 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.839\n",
      "Epoch: 331 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.832\n",
      "Epoch: 330 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.834\n",
      "Epoch: 329 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.832\n",
      "Epoch: 328 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.828\n",
      "Epoch: 327 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.829\n",
      "Epoch: 326 | T_Loss 0.689 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 0.826\n",
      "Epoch: 325 | T_Loss 0.691 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 0.824\n",
      "Epoch: 324 | T_Loss 0.689 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 0.821\n",
      "Epoch: 323 | T_Loss 0.689 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 0.821\n",
      "Epoch: 322 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.817\n",
      "Epoch: 321 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.821\n",
      "Epoch: 320 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.817\n",
      "Epoch: 319 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.817\n",
      "Epoch: 318 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 |  0.81\n",
      "Epoch: 317 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.811\n",
      "Epoch: 316 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.809\n",
      "Epoch: 315 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.806\n",
      "Epoch: 314 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.805\n",
      "Epoch: 313 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.801\n",
      "Epoch: 312 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.799\n",
      "Epoch: 311 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.799\n",
      "Epoch: 310 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.797\n",
      "Epoch: 309 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.793\n",
      "Epoch: 308 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.794\n",
      "Epoch: 307 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.794\n",
      "Epoch: 306 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 |  0.79\n",
      "Epoch: 305 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.786\n",
      "Epoch: 304 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.788\n",
      "Epoch: 303 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.783\n",
      "Epoch: 302 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.784\n",
      "Epoch: 301 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.783\n",
      "Epoch: 300 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.777\n",
      "Epoch: 299 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |  0.78\n",
      "Epoch: 298 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.775\n",
      "Epoch: 297 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.774\n",
      "Epoch: 296 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.772\n",
      "Epoch: 295 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.771\n",
      "Epoch: 294 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.769\n",
      "Epoch: 293 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.767\n",
      "Epoch: 292 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.768\n",
      "Epoch: 291 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.761\n",
      "Epoch: 290 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.758\n",
      "Epoch: 289 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.757\n",
      "Epoch: 288 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.758\n",
      "Epoch: 287 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.756\n",
      "Epoch: 286 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.757\n",
      "Epoch: 285 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.751\n",
      "Epoch: 284 | T_Loss 0.685 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.752\n",
      "Epoch: 283 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.745\n",
      "Epoch: 282 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.746\n",
      "Epoch: 281 | T_Loss 0.694 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.745\n",
      "Epoch: 280 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.746\n",
      "Epoch: 279 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.742\n",
      "Epoch: 278 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.738\n",
      "Epoch: 277 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.738\n",
      "Epoch: 276 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.736\n",
      "Epoch: 275 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.735\n",
      "Epoch: 274 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.732\n",
      "Epoch: 273 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 |  0.73\n",
      "Epoch: 272 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.728\n",
      "Epoch: 271 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.723\n",
      "Epoch: 270 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.725\n",
      "Epoch: 269 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.723\n",
      "Epoch: 268 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.721\n",
      "Epoch: 267 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.719\n",
      "Epoch: 266 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.717\n",
      "Epoch: 265 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.715\n",
      "Epoch: 264 | T_Loss 0.691 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 0.711\n",
      "Epoch: 263 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.712\n",
      "Epoch: 262 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.709\n",
      "Epoch: 261 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.707\n",
      "Epoch: 260 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.703\n",
      "Epoch: 259 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.608 | V_Acc   1.0 | 0.704\n",
      "Epoch: 258 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.703\n",
      "Epoch: 257 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.701\n",
      "Epoch: 256 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.696\n",
      "Epoch: 255 | T_Loss 0.691 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 0.696\n",
      "Epoch: 254 | T_Loss 0.689 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 0.696\n",
      "Epoch: 253 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.695\n",
      "Epoch: 252 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.691\n",
      "Epoch: 251 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.693\n",
      "Epoch: 250 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.683\n",
      "Epoch: 249 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.685\n",
      "Epoch: 248 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.687\n",
      "Epoch: 247 | T_Loss 0.685 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.678\n",
      "Epoch: 246 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.681\n",
      "Epoch: 245 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.679\n",
      "Epoch: 244 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.669\n",
      "Epoch: 243 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.676\n",
      "Epoch: 242 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.677\n",
      "Epoch: 241 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.673\n",
      "Epoch: 240 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.669\n",
      "Epoch: 239 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.663\n",
      "Epoch: 238 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 |  0.67\n",
      "Epoch: 237 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.664\n",
      "Epoch: 236 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.661\n",
      "Epoch: 235 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.661\n",
      "Epoch: 234 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 |  0.66\n",
      "Epoch: 233 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.657\n",
      "Epoch: 232 | T_Loss 0.684 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.656\n",
      "Epoch: 231 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.652\n",
      "Epoch: 230 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.653\n",
      "Epoch: 229 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.652\n",
      "Epoch: 228 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.651\n",
      "Epoch: 227 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.649\n",
      "Epoch: 226 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.643\n",
      "Epoch: 225 | T_Loss  0.69 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 0.643\n",
      "Epoch: 224 | T_Loss 0.696 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 0.641\n",
      "Epoch: 223 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 |  0.64\n",
      "Epoch: 222 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.633\n",
      "Epoch: 221 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.633\n",
      "Epoch: 220 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.635\n",
      "Epoch: 219 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.634\n",
      "Epoch: 218 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.629\n",
      "Epoch: 217 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.631\n",
      "Epoch: 216 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.609 | V_Acc   1.0 | 0.627\n",
      "Epoch: 215 | T_Loss 0.685 | T_Acc 0.542 | V_Loss  0.61 | V_Acc   1.0 | 0.625\n",
      "Epoch: 214 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.611 | V_Acc   1.0 | 0.621\n",
      "Epoch: 213 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.619\n",
      "Epoch: 212 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.612 | V_Acc   1.0 | 0.617\n",
      "Epoch: 211 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.613 | V_Acc   1.0 | 0.617\n",
      "Epoch: 210 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.617\n",
      "Epoch: 209 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.615\n",
      "Epoch: 208 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 0.609\n",
      "Epoch: 207 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.618 | V_Acc   1.0 | 0.613\n",
      "Epoch: 206 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.619 | V_Acc   1.0 | 0.605\n",
      "Epoch: 205 | T_Loss 0.689 | T_Acc 0.542 | V_Loss  0.62 | V_Acc   1.0 | 0.611\n",
      "Epoch: 204 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.622 | V_Acc   1.0 | 0.601\n",
      "Epoch: 203 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.625 | V_Acc   1.0 | 0.605\n",
      "Epoch: 202 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.628 | V_Acc   1.0 | 0.601\n",
      "Epoch: 201 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.631 | V_Acc   1.0 |   0.6\n",
      "Epoch: 200 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.634 | V_Acc   1.0 | 0.595\n",
      "Epoch: 199 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.637 | V_Acc   1.0 | 0.592\n",
      "Epoch: 198 | T_Loss 0.691 | T_Acc 0.542 | V_Loss  0.64 | V_Acc   1.0 |  0.59\n",
      "Epoch: 197 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.642 | V_Acc   1.0 |  0.59\n",
      "Epoch: 196 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.643 | V_Acc   1.0 | 0.591\n",
      "Epoch: 195 | T_Loss 0.695 | T_Acc 0.542 | V_Loss 0.644 | V_Acc   1.0 | 0.583\n",
      "Epoch: 194 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.642 | V_Acc   1.0 | 0.583\n",
      "Epoch: 193 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.637 | V_Acc   1.0 |  0.58\n",
      "Epoch: 192 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.626 | V_Acc   1.0 | 0.578\n",
      "Epoch: 191 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.607 | V_Acc   1.0 |  0.58\n",
      "Epoch: 190 | T_Loss 0.698 | T_Acc 0.542 | V_Loss 0.575 | V_Acc   1.0 |  0.58\n",
      "Epoch: 189 | T_Loss 0.714 | T_Acc 0.542 | V_Loss 0.518 | V_Acc   1.0 | 0.573\n",
      "Epoch: 188 | T_Loss 0.781 | T_Acc 0.542 | V_Loss  0.42 | V_Acc   1.0 | 0.571\n",
      "Epoch: 187 | T_Loss 0.952 | T_Acc 0.542 | V_Loss 0.271 | V_Acc   1.0 |  0.57\n",
      "Epoch: 186 | T_Loss 1.162 | T_Acc 0.542 | V_Loss 0.142 | V_Acc   1.0 | 0.571\n",
      "Epoch: 185 | T_Loss 1.375 | T_Acc 0.542 | V_Loss 0.078 | V_Acc   1.0 | 0.571\n",
      "Epoch: 184 | T_Loss 1.535 | T_Acc 0.542 | V_Loss 0.045 | V_Acc   1.0 | 0.565\n",
      "Epoch: 183 | T_Loss  1.62 | T_Acc 0.542 | V_Loss 0.034 | V_Acc   1.0 | 0.562\n",
      "Epoch: 182 | T_Loss 1.686 | T_Acc 0.542 | V_Loss 0.029 | V_Acc   1.0 | 0.559\n",
      "Epoch: 181 | T_Loss 1.733 | T_Acc 0.542 | V_Loss 0.025 | V_Acc   1.0 | 0.561\n",
      "Epoch: 180 | T_Loss 1.781 | T_Acc 0.542 | V_Loss 0.022 | V_Acc   1.0 | 0.558\n",
      "Epoch: 179 | T_Loss 1.855 | T_Acc   0.5 | V_Loss  0.02 | V_Acc   1.0 | 0.555\n",
      "Epoch: 178 | T_Loss 0.492 | T_Acc 0.708 | V_Loss 0.309 | V_Acc 0.667 | 0.552\n",
      "Epoch: 177 | T_Loss 0.502 | T_Acc 0.708 | V_Loss 0.878 | V_Acc   0.0 | 0.549\n",
      "Epoch: 176 | T_Loss 0.503 | T_Acc 0.708 | V_Loss 0.934 | V_Acc   0.0 | 0.549\n",
      "Epoch: 175 | T_Loss 0.505 | T_Acc 0.708 | V_Loss 0.937 | V_Acc   0.0 | 0.547\n",
      "Epoch: 174 | T_Loss 0.505 | T_Acc 0.708 | V_Loss 0.939 | V_Acc   0.0 | 0.545\n",
      "Epoch: 173 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.939 | V_Acc   0.0 | 0.544\n",
      "Epoch: 172 | T_Loss 0.503 | T_Acc 0.708 | V_Loss 0.939 | V_Acc   0.0 | 0.539\n",
      "Epoch: 171 | T_Loss 0.503 | T_Acc 0.708 | V_Loss 0.939 | V_Acc   0.0 | 0.542\n",
      "Epoch: 170 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.939 | V_Acc   0.0 | 0.536\n",
      "Epoch: 169 | T_Loss 0.505 | T_Acc 0.708 | V_Loss  0.94 | V_Acc   0.0 | 0.536\n",
      "Epoch: 168 | T_Loss 0.507 | T_Acc 0.708 | V_Loss  0.94 | V_Acc   0.0 | 0.532\n",
      "Epoch: 167 | T_Loss 0.507 | T_Acc 0.708 | V_Loss 0.941 | V_Acc   0.0 | 0.534\n",
      "Epoch: 166 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.941 | V_Acc   0.0 | 0.526\n",
      "Epoch: 165 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.527\n",
      "Epoch: 164 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.525\n",
      "Epoch: 163 | T_Loss 0.505 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.524\n",
      "Epoch: 162 | T_Loss 0.505 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.522\n",
      "Epoch: 161 | T_Loss 0.507 | T_Acc 0.708 | V_Loss 0.943 | V_Acc   0.0 | 0.517\n",
      "Epoch: 160 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.943 | V_Acc   0.0 |  0.52\n",
      "Epoch: 159 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.943 | V_Acc   0.0 | 0.518\n",
      "Epoch: 158 | T_Loss 0.505 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.516\n",
      "Epoch: 157 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 |  0.51\n",
      "Epoch: 156 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.509\n",
      "Epoch: 155 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.508\n",
      "Epoch: 154 | T_Loss 0.507 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.507\n",
      "Epoch: 153 | T_Loss 0.505 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.502\n",
      "Epoch: 152 | T_Loss 0.507 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.503\n",
      "Epoch: 151 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.499\n",
      "Epoch: 150 | T_Loss 0.505 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.497\n",
      "Epoch: 149 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.497\n",
      "Epoch: 148 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.943 | V_Acc   0.0 | 0.492\n",
      "Epoch: 147 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.943 | V_Acc   0.0 | 0.488\n",
      "Epoch: 146 | T_Loss 0.507 | T_Acc 0.708 | V_Loss 0.943 | V_Acc   0.0 |  0.49\n",
      "Epoch: 145 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.943 | V_Acc   0.0 | 0.486\n",
      "Epoch: 144 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.943 | V_Acc   0.0 | 0.489\n",
      "Epoch: 143 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.475\n",
      "Epoch: 142 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.481\n",
      "Epoch: 141 | T_Loss  0.51 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.478\n",
      "Epoch: 140 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.945 | V_Acc   0.0 | 0.476\n",
      "Epoch: 139 | T_Loss 0.507 | T_Acc 0.708 | V_Loss 0.945 | V_Acc   0.0 | 0.474\n",
      "Epoch: 138 | T_Loss 0.506 | T_Acc 0.708 | V_Loss 0.945 | V_Acc   0.0 | 0.475\n",
      "Epoch: 137 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.473\n",
      "Epoch: 136 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.468\n",
      "Epoch: 135 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.469\n",
      "Epoch: 134 | T_Loss 0.507 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.466\n",
      "Epoch: 133 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.464\n",
      "Epoch: 132 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.462\n",
      "Epoch: 131 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.458\n",
      "Epoch: 130 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.458\n",
      "Epoch: 129 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.455\n",
      "Epoch: 128 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.944 | V_Acc   0.0 | 0.454\n",
      "Epoch: 127 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.943 | V_Acc   0.0 | 0.452\n",
      "Epoch: 126 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.943 | V_Acc   0.0 | 0.449\n",
      "Epoch: 125 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.943 | V_Acc   0.0 | 0.447\n",
      "Epoch: 124 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.446\n",
      "Epoch: 123 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.942 | V_Acc   0.0 | 0.444\n",
      "Epoch: 122 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.941 | V_Acc   0.0 | 0.441\n",
      "Epoch: 121 | T_Loss 0.508 | T_Acc 0.708 | V_Loss  0.94 | V_Acc   0.0 |  0.44\n",
      "Epoch: 120 | T_Loss 0.509 | T_Acc 0.708 | V_Loss  0.94 | V_Acc   0.0 | 0.437\n",
      "Epoch: 119 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.939 | V_Acc   0.0 | 0.435\n",
      "Epoch: 118 | T_Loss  0.51 | T_Acc 0.708 | V_Loss 0.938 | V_Acc   0.0 | 0.436\n",
      "Epoch: 117 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.937 | V_Acc   0.0 | 0.434\n",
      "Epoch: 116 | T_Loss 0.508 | T_Acc 0.708 | V_Loss 0.936 | V_Acc   0.0 | 0.429\n",
      "Epoch: 115 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.936 | V_Acc   0.0 | 0.428\n",
      "Epoch: 114 | T_Loss  0.51 | T_Acc 0.708 | V_Loss 0.935 | V_Acc   0.0 | 0.427\n",
      "Epoch: 113 | T_Loss 0.509 | T_Acc 0.708 | V_Loss 0.934 | V_Acc   0.0 | 0.425\n",
      "Epoch: 112 | T_Loss  0.51 | T_Acc 0.708 | V_Loss 0.933 | V_Acc   0.0 | 0.422\n",
      "Epoch: 111 | T_Loss 0.511 | T_Acc 0.708 | V_Loss 0.933 | V_Acc   0.0 | 0.424\n",
      "Epoch: 110 | T_Loss  0.51 | T_Acc 0.708 | V_Loss 0.932 | V_Acc   0.0 | 0.419\n",
      "Epoch: 109 | T_Loss  0.51 | T_Acc 0.708 | V_Loss 0.931 | V_Acc   0.0 | 0.417\n",
      "Epoch: 108 | T_Loss 0.511 | T_Acc 0.708 | V_Loss  0.93 | V_Acc   0.0 | 0.417\n",
      "Epoch: 107 | T_Loss 0.511 | T_Acc 0.708 | V_Loss 0.928 | V_Acc   0.0 | 0.414\n",
      "Epoch: 106 | T_Loss  0.51 | T_Acc 0.708 | V_Loss 0.927 | V_Acc   0.0 | 0.413\n",
      "Epoch: 105 | T_Loss  0.51 | T_Acc 0.708 | V_Loss 0.925 | V_Acc   0.0 | 0.406\n",
      "Epoch: 104 | T_Loss 0.511 | T_Acc 0.708 | V_Loss 0.923 | V_Acc   0.0 | 0.408\n",
      "Epoch: 103 | T_Loss 0.511 | T_Acc 0.708 | V_Loss 0.921 | V_Acc   0.0 | 0.398\n",
      "Epoch: 102 | T_Loss 0.512 | T_Acc 0.708 | V_Loss 0.919 | V_Acc   0.0 | 0.404\n",
      "Epoch: 101 | T_Loss 0.513 | T_Acc 0.708 | V_Loss 0.917 | V_Acc   0.0 | 0.402\n",
      "Epoch: 100 | T_Loss 0.512 | T_Acc 0.708 | V_Loss 0.915 | V_Acc   0.0 | 0.401\n",
      "Epoch:  99 | T_Loss 0.512 | T_Acc 0.708 | V_Loss 0.912 | V_Acc   0.0 | 0.399\n",
      "Epoch:  98 | T_Loss 0.512 | T_Acc 0.708 | V_Loss  0.91 | V_Acc   0.0 | 0.388\n",
      "Epoch:  97 | T_Loss 0.512 | T_Acc 0.708 | V_Loss 0.907 | V_Acc   0.0 | 0.392\n",
      "Epoch:  96 | T_Loss 0.513 | T_Acc 0.708 | V_Loss 0.904 | V_Acc   0.0 | 0.394\n",
      "Epoch:  95 | T_Loss 0.514 | T_Acc 0.708 | V_Loss   0.9 | V_Acc   0.0 | 0.393\n",
      "Epoch:  94 | T_Loss 0.514 | T_Acc 0.708 | V_Loss 0.897 | V_Acc   0.0 | 0.385\n",
      "Epoch:  93 | T_Loss 0.514 | T_Acc 0.708 | V_Loss 0.893 | V_Acc   0.0 | 0.384\n",
      "Epoch:  92 | T_Loss 0.515 | T_Acc 0.708 | V_Loss 0.889 | V_Acc   0.0 | 0.385\n",
      "Epoch:  91 | T_Loss 0.516 | T_Acc 0.708 | V_Loss 0.884 | V_Acc   0.0 | 0.384\n",
      "Epoch:  90 | T_Loss 0.517 | T_Acc 0.708 | V_Loss 0.879 | V_Acc   0.0 | 0.382\n",
      "Epoch:  89 | T_Loss 0.516 | T_Acc 0.708 | V_Loss 0.874 | V_Acc   0.0 | 0.372\n",
      "Epoch:  88 | T_Loss 0.516 | T_Acc 0.708 | V_Loss 0.868 | V_Acc   0.0 | 0.374\n",
      "Epoch:  87 | T_Loss 0.516 | T_Acc 0.708 | V_Loss 0.861 | V_Acc   0.0 | 0.376\n",
      "Epoch:  86 | T_Loss 0.518 | T_Acc 0.708 | V_Loss 0.855 | V_Acc   0.0 | 0.374\n",
      "Epoch:  85 | T_Loss 0.518 | T_Acc 0.708 | V_Loss 0.849 | V_Acc   0.0 | 0.372\n",
      "Epoch:  84 | T_Loss  0.52 | T_Acc 0.708 | V_Loss 0.842 | V_Acc   0.0 | 0.359\n",
      "Epoch:  83 | T_Loss 0.521 | T_Acc 0.708 | V_Loss 0.835 | V_Acc   0.0 | 0.356\n",
      "Epoch:  82 | T_Loss 0.523 | T_Acc 0.708 | V_Loss 0.829 | V_Acc   0.0 | 0.363\n",
      "Epoch:  81 | T_Loss 0.523 | T_Acc 0.708 | V_Loss 0.822 | V_Acc   0.0 | 0.361\n",
      "Epoch:  80 | T_Loss 0.526 | T_Acc 0.708 | V_Loss 0.815 | V_Acc   0.0 | 0.354\n",
      "Epoch:  79 | T_Loss 0.528 | T_Acc 0.708 | V_Loss 0.808 | V_Acc   0.0 | 0.357\n",
      "Epoch:  78 | T_Loss 0.528 | T_Acc 0.708 | V_Loss   0.8 | V_Acc   0.0 | 0.348\n",
      "Epoch:  77 | T_Loss 0.532 | T_Acc 0.708 | V_Loss 0.793 | V_Acc   0.0 | 0.344\n",
      "Epoch:  76 | T_Loss 0.534 | T_Acc 0.708 | V_Loss 0.785 | V_Acc   0.0 |  0.35\n",
      "Epoch:  75 | T_Loss 0.536 | T_Acc 0.708 | V_Loss 0.776 | V_Acc   0.0 | 0.349\n",
      "Epoch:  74 | T_Loss 0.539 | T_Acc 0.708 | V_Loss 0.768 | V_Acc   0.0 | 0.347\n",
      "Epoch:  73 | T_Loss 0.542 | T_Acc 0.708 | V_Loss 0.766 | V_Acc   0.0 |  0.34\n",
      "Epoch:  72 | T_Loss 0.545 | T_Acc 0.708 | V_Loss 0.763 | V_Acc   0.0 | 0.338\n",
      "Epoch:  71 | T_Loss 0.552 | T_Acc 0.667 | V_Loss 0.758 | V_Acc   0.0 | 0.332\n",
      "Epoch:  70 | T_Loss 0.573 | T_Acc 0.708 | V_Loss 0.744 | V_Acc   0.0 | 0.338\n",
      "Epoch:  69 | T_Loss 0.561 | T_Acc 0.667 | V_Loss 0.749 | V_Acc   0.0 | 0.331\n",
      "Epoch:  68 | T_Loss  0.58 | T_Acc 0.708 | V_Loss 0.739 | V_Acc   0.0 | 0.326\n",
      "Epoch:  67 | T_Loss 0.586 | T_Acc 0.708 | V_Loss 0.741 | V_Acc   0.0 | 0.324\n",
      "Epoch:  66 | T_Loss 0.604 | T_Acc 0.667 | V_Loss 0.738 | V_Acc   0.0 | 0.326\n",
      "Epoch:  65 | T_Loss 0.606 | T_Acc 0.667 | V_Loss 0.697 | V_Acc 0.333 |  0.32\n",
      "Epoch:  64 | T_Loss 0.599 | T_Acc 0.667 | V_Loss 0.697 | V_Acc 0.333 | 0.319\n",
      "Epoch:  63 | T_Loss 0.638 | T_Acc 0.625 | V_Loss 0.729 | V_Acc   0.0 | 0.319\n",
      "Epoch:  62 | T_Loss 0.634 | T_Acc 0.667 | V_Loss 0.729 | V_Acc   0.0 | 0.313\n",
      "Epoch:  61 | T_Loss 0.617 | T_Acc 0.458 | V_Loss 0.719 | V_Acc   0.0 | 0.315\n",
      "Epoch:  60 | T_Loss 0.631 | T_Acc 0.667 | V_Loss 0.661 | V_Acc 0.333 | 0.312\n",
      "Epoch:  59 | T_Loss 0.642 | T_Acc 0.708 | V_Loss 0.715 | V_Acc   0.0 | 0.308\n",
      "Epoch:  58 | T_Loss 0.644 | T_Acc   0.5 | V_Loss 0.708 | V_Acc   0.0 | 0.306\n",
      "Epoch:  57 | T_Loss 0.651 | T_Acc 0.542 | V_Loss 0.685 | V_Acc 0.333 | 0.307\n",
      "Epoch:  56 | T_Loss 0.659 | T_Acc 0.583 | V_Loss 0.677 | V_Acc 0.667 | 0.303\n",
      "Epoch:  55 | T_Loss 0.669 | T_Acc 0.542 | V_Loss 0.662 | V_Acc   1.0 |   0.3\n",
      "Epoch:  54 | T_Loss 0.676 | T_Acc 0.542 | V_Loss 0.644 | V_Acc   1.0 | 0.298\n",
      "Epoch:  53 | T_Loss 0.681 | T_Acc 0.542 | V_Loss 0.628 | V_Acc   1.0 | 0.295\n",
      "Epoch:  52 | T_Loss 0.684 | T_Acc 0.542 | V_Loss 0.621 | V_Acc   1.0 | 0.295\n",
      "Epoch:  51 | T_Loss 0.684 | T_Acc 0.542 | V_Loss 0.619 | V_Acc   1.0 | 0.291\n",
      "Epoch:  50 | T_Loss 0.686 | T_Acc 0.542 | V_Loss 0.618 | V_Acc   1.0 | 0.291\n",
      "Epoch:  49 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 |  0.29\n",
      "Epoch:  48 | T_Loss 0.687 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 0.286\n",
      "Epoch:  47 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.619 | V_Acc   1.0 | 0.284\n",
      "Epoch:  46 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.619 | V_Acc   1.0 | 0.284\n",
      "Epoch:  45 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.619 | V_Acc   1.0 |  0.28\n",
      "Epoch:  44 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.618 | V_Acc   1.0 | 0.279\n",
      "Epoch:  43 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.618 | V_Acc   1.0 | 0.278\n",
      "Epoch:  42 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.618 | V_Acc   1.0 | 0.274\n",
      "Epoch:  41 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.618 | V_Acc   1.0 | 0.272\n",
      "Epoch:  40 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 0.271\n",
      "Epoch:  39 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 0.268\n",
      "Epoch:  38 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.267\n",
      "Epoch:  37 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.265\n",
      "Epoch:  36 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 0.264\n",
      "Epoch:  35 | T_Loss 0.689 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 |  0.26\n",
      "Epoch:  34 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 0.259\n",
      "Epoch:  33 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.617 | V_Acc   1.0 | 0.256\n",
      "Epoch:  32 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 0.255\n",
      "Epoch:  31 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.253\n",
      "Epoch:  30 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.251\n",
      "Epoch:  29 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.249\n",
      "Epoch:  28 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.248\n",
      "Epoch:  27 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.245\n",
      "Epoch:  26 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.614 | V_Acc   1.0 | 0.243\n",
      "Epoch:  25 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.615 | V_Acc   1.0 | 0.241\n",
      "Epoch:  24 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.616 | V_Acc   1.0 | 0.239\n",
      "Epoch:  23 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.619 | V_Acc   1.0 | 0.238\n",
      "Epoch:  22 | T_Loss 0.688 | T_Acc 0.542 | V_Loss 0.624 | V_Acc   1.0 | 0.235\n",
      "Epoch:  21 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.632 | V_Acc   1.0 | 0.233\n",
      "Epoch:  20 | T_Loss  0.69 | T_Acc 0.542 | V_Loss 0.641 | V_Acc   1.0 | 0.232\n",
      "Epoch:  19 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.652 | V_Acc   1.0 |  0.23\n",
      "Epoch:  18 | T_Loss 0.691 | T_Acc 0.542 | V_Loss 0.663 | V_Acc   1.0 | 0.228\n",
      "Epoch:  17 | T_Loss 0.692 | T_Acc 0.542 | V_Loss 0.674 | V_Acc   1.0 | 0.226\n",
      "Epoch:  16 | T_Loss 0.693 | T_Acc 0.542 | V_Loss 0.683 | V_Acc   1.0 | 0.224\n",
      "Epoch:  15 | T_Loss 0.693 | T_Acc   0.5 | V_Loss  0.69 | V_Acc   1.0 | 0.223\n",
      "Epoch:  14 | T_Loss 0.694 | T_Acc 0.458 | V_Loss 0.697 | V_Acc   0.0 | 0.221\n",
      "Epoch:  13 | T_Loss 0.694 | T_Acc 0.458 | V_Loss 0.702 | V_Acc   0.0 | 0.218\n",
      "Epoch:  12 | T_Loss 0.695 | T_Acc 0.458 | V_Loss 0.707 | V_Acc   0.0 | 0.217\n",
      "Epoch:  11 | T_Loss 0.695 | T_Acc 0.458 | V_Loss 0.711 | V_Acc   0.0 | 0.216\n",
      "Epoch:  10 | T_Loss 0.695 | T_Acc 0.458 | V_Loss 0.715 | V_Acc   0.0 | 0.213\n",
      "Epoch:   9 | T_Loss 0.695 | T_Acc 0.458 | V_Loss 0.718 | V_Acc   0.0 | 0.212\n",
      "Epoch:   8 | T_Loss 0.696 | T_Acc 0.458 | V_Loss 0.722 | V_Acc   0.0 |  0.21\n",
      "Epoch:   7 | T_Loss 0.696 | T_Acc 0.458 | V_Loss 0.725 | V_Acc   0.0 | 0.208\n",
      "Epoch:   6 | T_Loss 0.697 | T_Acc 0.458 | V_Loss 0.728 | V_Acc   0.0 | 0.207\n",
      "Epoch:   5 | T_Loss 0.697 | T_Acc 0.458 | V_Loss 0.731 | V_Acc   0.0 | 0.205\n",
      "Epoch:   4 | T_Loss 0.698 | T_Acc 0.458 | V_Loss 0.734 | V_Acc   0.0 | 0.203\n",
      "Epoch:   3 | T_Loss 0.698 | T_Acc 0.458 | V_Loss 0.737 | V_Acc   0.0 | 0.201\n",
      "Epoch:   2 | T_Loss 0.698 | T_Acc 0.458 | V_Loss 0.739 | V_Acc   0.0 |   0.2\n",
      "Epoch:   1 | T_Loss 0.698 | T_Acc 0.458 | V_Loss 0.742 | V_Acc   0.0 | 0.316\n",
      "\n",
      " Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Training initialization\n",
    "init_model()\n",
    "init_epoch()\n",
    "init_log()\n",
    "maximum_epoch = EPOCH\n",
    "\n",
    "# Training iteration\n",
    "\n",
    "while epoch_not_finished():\n",
    "    start_time = time.time()\n",
    "\n",
    "    tloss, tacc = epoch(train_loader, mode = 'train')\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    record_train_log(tloss, tacc, time_taken)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vloss, vacc = epoch(val_loader, mode = 'val')\n",
    "        record_valid_log(vloss, vacc)\n",
    "\n",
    "    print_log()\n",
    "\n",
    "print('\\n Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc.: 0.0\n",
      "Test Loss: 0.7791\n"
     ]
    }
   ],
   "source": [
    "# 정확도 검증\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc = epoch(test_loader, mode = 'test')\n",
    "    test_acc = round(test_acc, 4)\n",
    "    test_loss = round(test_loss, 4)\n",
    "    print('Test Acc.: {}'.format(test_acc))\n",
    "    print('Test Loss: {}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1분 원본 영상으로 모델 테스트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장된 frame의 개수: 181\n"
     ]
    }
   ],
   "source": [
    "# 영상 resize 및 추출\n",
    "test_video_name = 'C_3_12_43_BU_SMC_10-14_12-17-14_CC_RGB_DF2_F2'\n",
    "#test_video_path = f'/content/drive/MyDrive/Colab_Notebooks/Anomaly Detection/{test_video_name}.mp4'\n",
    "test_video_path = f'/home/raccoon/jnwork/kimsungwook/zerobase_DL_project/source_code/model/{test_video_name}.mp4'\n",
    "cv2.destroyAllWindows()\n",
    "cap = cv2.VideoCapture(test_video_path)\n",
    "img_list = []\n",
    "\n",
    "if cap.isOpened():\n",
    "\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if ret:\n",
    "            img = cv2.resize(img, (640, 640))\n",
    "            img_list.append(img)\n",
    "            # cv2_imshow(img)\n",
    "            # cv2.waitKey(1)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print('저장된 frame의 개수: {}'.format(len(img_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1713501158.226075    3656 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1713501158.249397    6889 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.161.07), renderer: NVIDIA GeForce GTX 1080 Ti/PCIe/SSE2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시퀀스 데이터 분석 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 181/181 [00:13<00:00, 13.84it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Yolov5 + Mediapipe Version\"\"\"\n",
    "\n",
    "net.eval()\n",
    "\n",
    "length = 30 # frame 상태를 표시할 길이\n",
    "out_img_list = []\n",
    "dataset = []\n",
    "status = 'None'\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1, enable_segmentation=False, min_detection_confidence=n_CONFIDENCE)\n",
    "print('시퀀스 데이터 분석 중...')\n",
    "\n",
    "xy_list_list = []\n",
    "for img in tqdm(img_list):\n",
    "    res = yolo_model(img)\n",
    "    res_refine = res.pandas().xyxy[0].values\n",
    "\n",
    "    nms_human = len(res_refine)\n",
    "    if nms_human > 0:\n",
    "        for bbox in res_refine:\n",
    "            xx1, yy1, xx2, yy2 = int(bbox[0])-10, int(bbox[1]), int(bbox[2])+10, int(bbox[3])\n",
    "            if xx1 < 0:\n",
    "                xx1 = 0\n",
    "            elif xx2 > 639:\n",
    "                xx2 = 639\n",
    "            if yy1 < 0:\n",
    "                yy1 = 0\n",
    "            elif yy2 > 639:\n",
    "                yy2 = 639\n",
    "\n",
    "            start_point = (xx1, yy1)\n",
    "            end_point = (xx2, yy2)\n",
    "            if bbox[4] > y_CONFIDENCE:\n",
    "                img = cv2.rectangle(img, start_point, end_point, (0, 0, 255), 2)\n",
    "\n",
    "                c_img = img[yy1:yy2, xx1:xx2]\n",
    "                results = pose.process(cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)) # Yolo 바운딩 box 안에서 landmark dot 추출\n",
    "                if not results.pose_landmarks: continue\n",
    "                xy_list = []\n",
    "                idx = 0\n",
    "                draw_line_dic = {}\n",
    "                for x_and_y in results.pose_landmarks.landmark:\n",
    "                    if idx in attention_dot:\n",
    "                        xy_list.append(x_and_y.x)\n",
    "                        xy_list.append(x_and_y.y)\n",
    "                        x, y = int(x_and_y.x*(xx2-xx1)), int(x_and_y.y*(yy2-yy1))\n",
    "                        draw_line_dic[idx] = [x, y]\n",
    "                    idx += 1\n",
    "\n",
    "                xy_list_list.append(xy_list)\n",
    "                for line in draw_line:\n",
    "                    x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
    "                    x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
    "                    c_img = cv2.line(c_img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "                if len(xy_list_list) == length:\n",
    "                    dataset = []\n",
    "                    dataset.append({'key' : 0, 'value' : xy_list_list})\n",
    "                    dataset = MyDataset(dataset)\n",
    "                    dataset = DataLoader(dataset)\n",
    "                    xy_list_list = []\n",
    "\n",
    "                    for data, label in dataset:\n",
    "                        data = data.to(device)\n",
    "                        with torch.no_grad():\n",
    "                            result = net(data)\n",
    "                            _, out = torch.max(result, 1)\n",
    "                            if out.item() == 0: status = 'Normal'\n",
    "                            else: status = 'Theft'\n",
    "\n",
    "    cv2.putText(img, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 0, 255), 2)\n",
    "    out_img_list.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "# 테스트 원본 영상 내보내기\n",
    "filename = './output.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "fps = 3\n",
    "frameSize = (640, 640)\n",
    "isColor = True\n",
    "out = cv2.VideoWriter(filename, fourcc, fps, frameSize, isColor)\n",
    "for out_img in out_img_list:\n",
    "    out.write(out_img)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하기\n",
    "PATH = './'\n",
    "model_name = 'LSTM.pt'\n",
    "torch.save(net.state_dict(), PATH + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장된 frame의 개수: 1483\n"
     ]
    }
   ],
   "source": [
    "# 영상 resize 및 추출\n",
    "test_video_name = 'sample_1.mov'\n",
    "#test_video_path = f'/content/drive/MyDrive/Colab_Notebooks/Anomaly Detection/{test_video_name}'\n",
    "test_video_path = f'/home/raccoon/jnwork/kimsungwook/zerobase_DL_project/source_code/model/{test_video_name}'\n",
    "cv2.destroyAllWindows()\n",
    "cap = cv2.VideoCapture(test_video_path)\n",
    "img_list = []\n",
    "\n",
    "if cap.isOpened():\n",
    "\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "        if ret:\n",
    "            img = cv2.resize(img, (640, 640))\n",
    "            img_list.append(img)\n",
    "            # cv2_imshow(img)\n",
    "            # cv2.waitKey(1)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print('저장된 frame의 개수: {}'.format(len(img_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1713506886.337675    3656 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1713506886.362469   11895 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.161.07), renderer: NVIDIA GeForce GTX 1080 Ti/PCIe/SSE2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시퀀스 데이터 분석 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 1483/1483 [01:31<00:00, 16.28it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Yolov5 + Mediapipe Version\"\"\"\n",
    "\n",
    "net.eval()\n",
    "\n",
    "length = 30 # frame 상태를 표시할 길이\n",
    "out_img_list = []\n",
    "dataset = []\n",
    "status = 'None'\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1, enable_segmentation=False, min_detection_confidence=n_CONFIDENCE)\n",
    "print('시퀀스 데이터 분석 중...')\n",
    "\n",
    "xy_list_list = []\n",
    "for img in tqdm(img_list):\n",
    "    res = yolo_model(img)\n",
    "    res_refine = res.pandas().xyxy[0].values\n",
    "\n",
    "    nms_human = len(res_refine)\n",
    "    if nms_human > 0:\n",
    "        for bbox in res_refine:\n",
    "            xx1, yy1, xx2, yy2 = int(bbox[0])-10, int(bbox[1]), int(bbox[2])+10, int(bbox[3])\n",
    "            if xx1 < 0:\n",
    "                xx1 = 0\n",
    "            elif xx2 > 639:\n",
    "                xx2 = 639\n",
    "            if yy1 < 0:\n",
    "                yy1 = 0\n",
    "            elif yy2 > 639:\n",
    "                yy2 = 639\n",
    "\n",
    "            start_point = (xx1, yy1)\n",
    "            end_point = (xx2, yy2)\n",
    "            if bbox[4] > y_CONFIDENCE:\n",
    "                img = cv2.rectangle(img, start_point, end_point, (0, 0, 255), 2)\n",
    "\n",
    "                c_img = img[yy1:yy2, xx1:xx2]\n",
    "                results = pose.process(cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)) # Yolo 바운딩 box 안에서 landmark dot 추출\n",
    "                if not results.pose_landmarks: continue\n",
    "                xy_list = []\n",
    "                idx = 0\n",
    "                draw_line_dic = {}\n",
    "                for x_and_y in results.pose_landmarks.landmark:\n",
    "                    if idx in attention_dot:\n",
    "                        xy_list.append(x_and_y.x)\n",
    "                        xy_list.append(x_and_y.y)\n",
    "                        x, y = int(x_and_y.x*(xx2-xx1)), int(x_and_y.y*(yy2-yy1))\n",
    "                        draw_line_dic[idx] = [x, y]\n",
    "                    idx += 1\n",
    "\n",
    "                xy_list_list.append(xy_list)\n",
    "                for line in draw_line:\n",
    "                    x1, y1 = draw_line_dic[line[0]][0], draw_line_dic[line[0]][1]\n",
    "                    x2, y2 = draw_line_dic[line[1]][0], draw_line_dic[line[1]][1]\n",
    "                    c_img = cv2.line(c_img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "                if len(xy_list_list) == length:\n",
    "                    dataset = []\n",
    "                    dataset.append({'key' : 0, 'value' : xy_list_list})\n",
    "                    dataset = MyDataset(dataset)\n",
    "                    dataset = DataLoader(dataset)\n",
    "                    xy_list_list = []\n",
    "\n",
    "                    for data, label in dataset:\n",
    "                        data = data.to(device)\n",
    "                        with torch.no_grad():\n",
    "                            result = net(data)\n",
    "                            _, out = torch.max(result, 1)\n",
    "                            if out.item() == 0: status = 'Normal'\n",
    "                            else: status = 'Theft'\n",
    "\n",
    "    cv2.putText(img, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 0, 255), 2)\n",
    "    out_img_list.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "# 테스트 원본 영상 내보내기\n",
    "filename = './output2.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "fps = 3\n",
    "frameSize = (640, 640)\n",
    "isColor = True\n",
    "out = cv2.VideoWriter(filename, fourcc, fps, frameSize, isColor)\n",
    "for out_img in out_img_list:\n",
    "    out.write(out_img)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
